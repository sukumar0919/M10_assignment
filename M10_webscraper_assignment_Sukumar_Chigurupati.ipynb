{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36b58323",
   "metadata": {},
   "source": [
    "## Module 10 Assignment - Scraping a Website\n",
    "* Author: brandon chiazza\n",
    "* version 2.0\n",
    "\n",
    "We will be creating a web scraper to parse a table from the Charities Bureau Website. From the website: “All \n",
    "charitable organizations operating in New York State are required by law to register and file annual financial reports \n",
    "with the Attorney General's Office. This includes any organization that conducts charitable activities, holds property \n",
    "that is used for charitable purposes, or solicits financial or other contributions.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a378dd19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully uploaded file to location: s3://m10assignment/charities_bureau_scrape_20240420074402.csv\n"
     ]
    }
   ],
   "source": [
    "### Load modules\n",
    "#!pip install webdriver-manager\n",
    "#!pip install awscli\n",
    "import selenium\n",
    "import pandas as pd\n",
    "import time\n",
    "from time import sleep\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "#### SCRAPE THE WEBSITE ######\n",
    "### Call the webdriver\n",
    "s = Service(ChromeDriverManager().install())\n",
    "browser = webdriver.Chrome(service=s)\n",
    "\n",
    "# Enter the URL path that needs to be accessed by webdriver\n",
    "browser.get('https://www.charitiesnys.com/RegistrySearch/search_charities.jsp')\n",
    "\n",
    "# Identify xpath of location to select element\n",
    "inputElement = browser.find_element(By.XPATH,'//*[@id=\"header\"]/div[2]/div/table/tbody/tr/td[2]/div/div/font/font/font/font/font/font/table/tbody/tr[4]/td/form/table/tbody/tr[2]/td[2]/input[1]') #identifies the location of the EIN element\n",
    "inputElement.send_keys('0') #sends the \"0\" as the search value for EIN \n",
    "inputElement1 = browser.find_element(By.XPATH,'//*[@id=\"header\"]/div[2]/div/table/tbody/tr/td[2]/div/div/font/font/font/font/font/font/table/tbody/tr[4]/td/form/table/tbody/tr[10]/td/input[1]').click() #instatiates the click of the search\n",
    "sleep(4) #allow for the page to load by adding a sleep element\n",
    "# Identify the table to scrape\n",
    "table = browser.find_element(By.CSS_SELECTOR,'table.Bordered')\n",
    "sleep(1)\n",
    "\n",
    "##### CREATE DATA FRAME #####\n",
    "# Create empty list to store data\n",
    "data = []\n",
    "\n",
    "# Loop through table rows to extract data and remove blank rows\n",
    "for row in table.find_elements(By.CSS_SELECTOR, 'tr'):\n",
    "    # Extract text from cells in the row\n",
    "    row_data = [cell.text for cell in row.find_elements(By.CSS_SELECTOR, 'td')]\n",
    "    # Check if the row contains data\n",
    "    if row_data:\n",
    "        data.append(row_data)\n",
    "\n",
    "# Convert list to DataFrame\n",
    "df = pd.DataFrame(data, columns=[\"Organization Name\", \"NY Reg #\", \"EIN\", \"Registrant Type\", \"City\", \"State\"])\n",
    "\n",
    "### LOAD THE FILE INTO S3 ####\n",
    "# Prepare csv file name   \n",
    "bucket_name = 'm10assignment'  # specify your S3 bucket name\n",
    "pathname = f's3://{bucket_name}/'  # specify location of s3://{my-bucket}/\n",
    "filename = 'charities_bureau_scrape_'  # name of your file\n",
    "datetime = time.strftime(\"%Y%m%d%H%M%S\")  # timestamp\n",
    "filenames3 = f\"{pathname}{filename}{datetime}.csv\"  # name of the filepath and csv file\n",
    "\n",
    "# Load file into S3. Pandas actually leverages boto to connect to S3 and can push the file directly into an S3 bucket\n",
    "df.to_csv(filenames3, header=True, index=False) \n",
    "\n",
    "# Print success message\n",
    "print(\"Successfully uploaded file to location:\", filenames3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "81db30e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully uploaded file to location: s3://m10assignment/charities_bureau_scrape_20240420074406.csv\n"
     ]
    }
   ],
   "source": [
    "###LOAD THE FILE INTO S3####\n",
    "# Prepare csv file name   \n",
    "bucket_name = 'm10assignment'  # specify your S3 bucket name\n",
    "pathname = f's3://{bucket_name}/'  # specify location of s3://{my-bucket}/\n",
    "filename = 'charities_bureau_scrape_'  # name of your group\n",
    "datetime = time.strftime(\"%Y%m%d%H%M%S\")  # timestamp\n",
    "filenames3 = f\"{pathname}{filename}{datetime}.csv\"  # name of the filepath and csv file\n",
    "\n",
    "# Load file into S3. Pandas actually leverages boto to connect to S3 and can push the file directly into an S3 bucket\n",
    "df.to_csv(filenames3, header=True, index=False)\n",
    "\n",
    "# Print success message\n",
    "print(\"Successfully uploaded file to location:\", filenames3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befb7934",
   "metadata": {},
   "source": [
    "## References\n",
    "* https://www.programiz.com/python-programming/working-csv-files\n",
    "* https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#S3.Client.create_bucket\n",
    "* https://realpython.com/python-boto3-aws-s3/\n",
    "* https://robertorocha.info/setting-up-a-selenium-web-scraper-on-aws-lambda-with-python/ "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
